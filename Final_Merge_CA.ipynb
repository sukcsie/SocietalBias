{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data: \n",
    "\n",
    "The data for all of the cities used here is available through the Stanford Open Policing Project. The data can be found here: https://openpolicing.stanford.edu/data/\n",
    "\n",
    "For this project our goal was to use all available municiple police stops data from cities in California . We excluded Anaheim and San Bernardino CA, because the driver race for the stops was not recorded, and this was important for our analysis. In our final notebook, we loaded data from 9 cities, Oakland, San Francisco, San Jose, Bakersfield, Long Beach, Los Angeles, San Diego, Santa Ana, and Stockton. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "df_o = pd.read_csv('Data/ca_oakland_2020_04_01.csv', low_memory=False)\n",
    "df_sf = pd.read_csv('Data/ca_san_francisco_2020_04_01.csv',low_memory=False)\n",
    "df_sj = pd.read_csv('Data/ca_san_jose_2020_04_01.csv',low_memory=False)\n",
    "df_b = pd.read_csv('Data/ca_bakersfield_2020_04_01.csv',low_memory=False)\n",
    "df_lb = pd.read_csv('Data/ca_long_beach_2020_04_01.csv',low_memory=False)\n",
    "df_la = pd.read_csv('Data/ca_los_angeles_2020_04_01.csv',low_memory=False)\n",
    "df_sd = pd.read_csv('Data/ca_san_diego_2020_04_01.csv',low_memory=False)\n",
    "df_sa = pd.read_csv('Data/ca_santa_ana_2020_04_01.csv',low_memory=False)\n",
    "df_s = pd.read_csv('Data/ca_stockton_2020_04_01.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different cities have provided different columns of data. I will create a hash table where the keys are the different variable columns, and the definition is the dataset name. This will allow me to print a nice list of each column, and which city datasets have that variable. This will help us see which cities we can use for the different types of analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the column rows of each dataset\n",
    "# Getting a list of the names of the datasets\n",
    "\n",
    "o_columns = df_o.columns.tolist()\n",
    "sf_columns = df_sf.columns.tolist()\n",
    "sj_columns = df_sj.columns.tolist()\n",
    "b_columns = df_b.columns.tolist()\n",
    "lb_columns = df_lb.columns.tolist()\n",
    "la_columns = df_la.columns.tolist()\n",
    "sd_columns = df_sd.columns.tolist()\n",
    "sa_columns = df_sa.columns.tolist()\n",
    "s_columns = df_s.columns.tolist()\n",
    "\n",
    "# Now getting a list of lists, where each list element is the columns of one of the datasets\n",
    "df_names = ['df_o','df_sf','df_sj','df_b','df_lb','df_la','df_sd','df_sa','df_s']\n",
    "column_names = [o_columns]+[sf_columns]+[sj_columns]+[b_columns]+[lb_columns]+[la_columns]+[sd_columns]+[sa_columns]+[s_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to put the column names as keys into a hash table\n",
    "# the definition is the name of the datasets that have the column, so that we can see clearly which cities have what variables\n",
    "def create_col_names(column_names,df_names):\n",
    "    for i in range (0,len(column_names)): \n",
    "        for j in column_names[i]:\n",
    "            if j not in column_names_hash:\n",
    "                column_names_hash[j] = []\n",
    "            column_names_hash[j] = column_names_hash[j] + [df_names[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function on our column_names and db_names variables\n",
    "column_names_hash = {}\n",
    "create_col_names(column_names,df_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date ['df_o', 'df_sf', 'df_sj', 'df_b', 'df_lb', 'df_la', 'df_sd', 'df_sa', 'df_s']\n",
      "time ['df_o', 'df_sf', 'df_sj', 'df_b', 'df_la', 'df_sd']\n",
      "location ['df_o', 'df_sf', 'df_sj', 'df_b', 'df_lb', 'df_sa']\n",
      "lat ['df_o', 'df_sf', 'df_sj', 'df_b', 'df_lb', 'df_sa']\n",
      "lng ['df_o', 'df_sf', 'df_sj', 'df_b', 'df_lb', 'df_sa']\n",
      "beat ['df_o', 'df_b', 'df_lb']\n",
      "subject_age ['df_o', 'df_sf', 'df_b', 'df_lb', 'df_sd', 'df_s']\n",
      "subject_race ['df_o', 'df_sf', 'df_sj', 'df_b', 'df_lb', 'df_la', 'df_sd', 'df_sa', 'df_s']\n",
      "subject_sex ['df_o', 'df_sf', 'df_b', 'df_lb', 'df_la', 'df_sd', 'df_sa', 'df_s']\n",
      "officer_assignment ['df_o']\n",
      "type ['df_o', 'df_sf', 'df_sj', 'df_b', 'df_lb', 'df_la', 'df_sd', 'df_sa', 'df_s']\n",
      "arrest_made ['df_o', 'df_sf', 'df_sj', 'df_sd', 'df_s']\n",
      "citation_issued ['df_o', 'df_sf', 'df_sj', 'df_b', 'df_lb', 'df_sd', 'df_sa', 'df_s']\n",
      "warning_issued ['df_o', 'df_sf', 'df_sd', 'df_s']\n",
      "outcome ['df_o', 'df_sf', 'df_sj', 'df_b', 'df_lb', 'df_sd', 'df_sa', 'df_s']\n",
      "contraband_found ['df_o', 'df_sf', 'df_sj', 'df_sd']\n",
      "contraband_drugs ['df_o']\n",
      "contraband_weapons ['df_o']\n",
      "search_conducted ['df_o', 'df_sf', 'df_sj', 'df_sd', 'df_s']\n",
      "search_basis ['df_o', 'df_sf', 'df_sd', 'df_s']\n",
      "reason_for_stop ['df_o', 'df_sf', 'df_sj', 'df_sd', 'df_s']\n",
      "use_of_force_description ['df_o', 'df_sj']\n",
      "district ['df_sf', 'df_lb', 'df_la', 'df_sa']\n",
      "search_vehicle ['df_sf', 'df_sd']\n",
      "use_of_force_reason ['df_sj']\n",
      "officer_id_hash ['df_b', 'df_lb', 'df_la', 'df_sa', 'df_s']\n",
      "subdistrict ['df_lb']\n",
      "division ['df_lb', 'df_s']\n",
      "officer_age ['df_lb']\n",
      "officer_race ['df_lb']\n",
      "officer_sex ['df_lb']\n",
      "officer_years_of_service ['df_lb']\n",
      "violation ['df_lb', 'df_sa']\n",
      "vehicle_make ['df_lb']\n",
      "vehicle_registration_state ['df_lb']\n",
      "vehicle_year ['df_lb']\n",
      "region ['df_la', 'df_sa']\n",
      "service_area ['df_sd']\n",
      "search_person ['df_sd']\n",
      "reason_for_search ['df_sd']\n"
     ]
    }
   ],
   "source": [
    "# printing the variables and which datasets have them\n",
    "for i in column_names_hash: \n",
    "    if 'raw' not in i: \n",
    "        print(i, column_names_hash[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The columns that are important to our research are:**\n",
    "\n",
    "* Date, Time, Age, Gender, Race, Type \n",
    "* We also decided to keep: Lat, Lng, Outcome, Search Conducted, Contraband Found we did not end up using them in our analysis for this paper, but we thought they might be useful for further research. \n",
    "\n",
    "* For any datasets that are missing these columns, we will fill them in with N/A, so when we merge the datasets with an inner join the columns will remain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling in the columns we need in the datasets that don't have them with N/A\n",
    "important_variables = ['date','time','lat','lng','subject_age','subject_sex','subject_race','type','outcome','search_conducted','contraband_found']\n",
    "df_names1 = [df_o,df_sf,df_sj,df_b,df_lb,df_la,df_sd,df_sa,df_s]\n",
    "for df in df_names1: \n",
    "    for var in important_variables: \n",
    "        if var not in df: \n",
    "            df[var] = 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Assigning datasets with the new filled in versions\n",
    "df_o = df_names1[0]\n",
    "df_sf = df_names1[1]\n",
    "df_sj = df_names1[2]\n",
    "df_b = df_names1[3]\n",
    "df_lb = df_names1[4]\n",
    "df_la = df_names1[5]\n",
    "df_sd = df_names1[6]\n",
    "df_sa = df_names1[7]\n",
    "df_s = df_names1[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column for the city\n",
    "# so we will know what city each stop came from\n",
    "df_sj['city'] = 'San Jose'\n",
    "df_sf['city'] = 'San Francisco'\n",
    "df_o['city'] = 'Oakland'\n",
    "df_b['city'] = 'Bakersfield'\n",
    "df_lb['city'] = 'Long Beach'\n",
    "df_la['city'] = 'Los Angeles'\n",
    "df_sd['city'] = 'San Diego'\n",
    "df_sa['city'] = 'Santa Ana'\n",
    "df_s['city'] = 'Stockton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinating the datasets on an inner join, so only the columns that all of them have remain\n",
    "df_all = pd.concat([df_o,df_sf,df_sj,df_b,df_lb,df_la,df_sd,df_sa,df_s],join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'time', 'lat', 'lng', 'subject_age', 'subject_race',\n",
       "       'subject_sex', 'type', 'outcome', 'contraband_found',\n",
       "       'search_conducted', 'city'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the columns to make sure things were merged properly\n",
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can delete raw_row_number, because we can use the index to identify rows\n",
    "df_all = df_all.drop(columns=['raw_row_number'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the new merged dataset\n",
    "df_all.to_csv('Data/df_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
